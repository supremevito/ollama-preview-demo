# Ollama Python è°ƒç”¨ç¤ºä¾‹

è¿™æ˜¯ä¸€ä¸ªæœ€ç®€å•çš„ Python è°ƒç”¨æœ¬åœ° Ollama æ¥å£çš„ç¤ºä¾‹é¡¹ç›®ã€‚

## ğŸ“‹ å‰ç½®è¦æ±‚

1. **å®‰è£… Ollama**
   ```bash
   # macOS
   brew install ollama
   
   # æˆ–ä»å®˜ç½‘ä¸‹è½½
   # https://ollama.ai/download
   ```

2. **å¯åŠ¨ Ollama æœåŠ¡**
   ```bash
   ollama serve
   ```

3. **ä¸‹è½½æ¨¡å‹**
   ```bash
   # ä¸‹è½½ llama2 æ¨¡å‹ï¼ˆæ¨èï¼‰
   ollama pull llama2
   
   # æˆ–å…¶ä»–æ¨¡å‹
   ollama pull mistral
   ollama pull codellama
   ollama pull qwen
   ```

4. **å®‰è£… Python ä¾èµ–**
   ```bash
   pip install -r requirements.txt
   ```

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### æ–¹å¼ 1: ç®€å•ç¤ºä¾‹ï¼ˆollama_demo.pyï¼‰

åŒ…å«ä¸¤ç§è°ƒç”¨æ–¹å¼ï¼š
- æ™®é€šè°ƒç”¨ï¼šä¸€æ¬¡æ€§è¿”å›å®Œæ•´ç»“æœ
- æµå¼è°ƒç”¨ï¼šé€å­—è¾“å‡ºï¼ˆç±»ä¼¼ ChatGPT æ•ˆæœï¼‰

```bash
python ollama_demo.py
```

### æ–¹å¼ 2: äº¤äº’å¼èŠå¤©ï¼ˆollama_chat.pyï¼‰

æ”¯æŒè¿ç»­å¯¹è¯ï¼Œä¿ç•™ä¸Šä¸‹æ–‡ï¼š

```bash
python ollama_chat.py
```

**åŠŸèƒ½ï¼š**
- è¾“å…¥é—®é¢˜è¿›è¡Œå¯¹è¯
- è¾“å…¥ `clear` æ¸…ç©ºå¯¹è¯å†å²
- è¾“å…¥ `exit` æˆ– `quit` é€€å‡º

## ğŸ“¡ API è¯´æ˜

### 1. Generate APIï¼ˆç”Ÿæˆæ¥å£ï¼‰

**ç«¯ç‚¹ï¼š** `http://localhost:11434/api/generate`

**è¯·æ±‚ç¤ºä¾‹ï¼š**
```python
import requests

response = requests.post(
    "http://localhost:11434/api/generate",
    json={
        "model": "llama2",
        "prompt": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "stream": False
    }
)

result = response.json()
print(result["response"])
```

### 2. Chat APIï¼ˆå¯¹è¯æ¥å£ï¼‰

**ç«¯ç‚¹ï¼š** `http://localhost:11434/api/chat`

**è¯·æ±‚ç¤ºä¾‹ï¼š**
```python
import requests

response = requests.post(
    "http://localhost:11434/api/chat",
    json={
        "model": "llama2",
        "messages": [
            {"role": "user", "content": "ä½ å¥½"}
        ],
        "stream": False
    }
)

result = response.json()
print(result["message"]["content"])
```

## ğŸ”§ å¸¸ç”¨å‘½ä»¤

```bash
# æŸ¥çœ‹å·²å®‰è£…çš„æ¨¡å‹
ollama list

# æŸ¥çœ‹æ­£åœ¨è¿è¡Œçš„æ¨¡å‹
ollama ps

# åˆ é™¤æ¨¡å‹
ollama rm llama2

# æŸ¥çœ‹æ¨¡å‹ä¿¡æ¯
ollama show llama2
```

## ğŸ“ ä»£ç è¯´æ˜

### ollama_demo.py
- `chat_with_ollama()`: æ™®é€šè°ƒç”¨ï¼Œè¿”å›å®Œæ•´ç»“æœ
- `chat_with_ollama_stream()`: æµå¼è°ƒç”¨ï¼Œé€å­—è¾“å‡º

### ollama_chat.py
- äº¤äº’å¼èŠå¤©ç¨‹åº
- æ”¯æŒä¸Šä¸‹æ–‡å¯¹è¯
- å¯æ¸…ç©ºå†å²è®°å½•

## ğŸ¯ å¿«é€Ÿæµ‹è¯•

```python
import requests

# æœ€ç®€å•çš„è°ƒç”¨
response = requests.post(
    "http://localhost:11434/api/generate",
    json={
        "model": "llama2",
        "prompt": "Hello!",
        "stream": False
    }
)

print(response.json()["response"])
```

## âš ï¸ æ³¨æ„äº‹é¡¹

1. ç¡®ä¿ Ollama æœåŠ¡å·²å¯åŠ¨ï¼ˆ`ollama serve`ï¼‰
2. ç¡®ä¿å·²ä¸‹è½½å¯¹åº”çš„æ¨¡å‹ï¼ˆ`ollama pull llama2`ï¼‰
3. é»˜è®¤ç«¯å£æ˜¯ `11434`ï¼Œå¦‚æœä¿®æ”¹äº†ç«¯å£éœ€è¦ç›¸åº”è°ƒæ•´ä»£ç 
4. é¦–æ¬¡è¿è¡Œæ¨¡å‹ä¼šæ¯”è¾ƒæ…¢ï¼Œåç»­ä¼šå¿«å¾ˆå¤š

## ğŸŒŸ æ¨èæ¨¡å‹

| æ¨¡å‹ | å¤§å° | ç”¨é€” |
|------|------|------|
| llama2 | 3.8GB | é€šç”¨å¯¹è¯ |
| mistral | 4.1GB | é«˜è´¨é‡å¯¹è¯ |
| codellama | 3.8GB | ä»£ç ç”Ÿæˆ |
| qwen | 4.7GB | ä¸­æ–‡å¯¹è¯ |
| llama2-chinese | 3.8GB | ä¸­æ–‡ä¼˜åŒ– |

## ğŸ“š æ›´å¤šèµ„æº

- [Ollama å®˜ç½‘](https://ollama.ai/)
- [Ollama GitHub](https://github.com/ollama/ollama)
- [API æ–‡æ¡£](https://github.com/ollama/ollama/blob/main/docs/api.md)

